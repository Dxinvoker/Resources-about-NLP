# **Resources-about-NLP** 

## Machine Learning

[决策树算法原理(上)](https://www.cnblogs.com/pinard/p/6050306.html)

[决策树算法原理(下)](https://www.cnblogs.com/pinard/p/6053344.html)

[这可能是你看过的最用心的【决策树算法】介绍文章](https://zhuanlan.zhihu.com/p/32053821)

[RF、GBDT、XGBoost面试级整理](https://blog.csdn.net/qq_28031525/article/details/70207918)

[通俗理解kaggle比赛大杀器xgboost](https://www.julyedu.com/question/big/kp_id/23/ques_id/2590)

[机器学习面试之各种优化器的比较](https://www.jianshu.com/p/ee39eca29117)

[简单的交叉熵损失函数，你真的懂了吗？](https://zhuanlan.zhihu.com/p/38241764)

[漫谈：机器学习中距离和相似性度量方法](https://www.cnblogs.com/daniel-D/p/3244718.html)

[EM-最大期望算法](http://www.csuldw.com/2015/12/02/2015-12-02-EM-algorithms/)

[为什么L1稀疏L2平滑？](https://blog.csdn.net/li8zi8fa/article/details/77649973)

## Deep Learning

#### RNN

[RNN](https://blog.csdn.net/zhaojc1995/article/details/80572098)

[RNN - LSTM - GRU](https://zhuanlan.zhihu.com/p/60915302)

[LSTM细节分析理解（pytorch版）](<https://zhuanlan.zhihu.com/p/79064602>)

[RNN/LSTM BPTT详细推导以及梯度消失问题分析](https://zhuanlan.zhihu.com/p/85776566)

[PyTorch 训练 RNN 时，序列长度不固定怎么办？](https://zhuanlan.zhihu.com/p/59772104)

[RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)

#### 文本分类

[在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的tricks？](https://www.zhihu.com/question/265357659)

#### 语义相似度

[深度学习解决 NLP 问题：语义相似度计算](https://cloud.tencent.com/developer/article/1005600)

[浅析文本相似度](https://blog.csdn.net/qq_28031525/article/details/79596376)

#### Seq2Seq & Transfomer & Attention

[真正的完全图解Seq2Seq Attention模型](https://zhuanlan.zhihu.com/p/40920384)

[Transformer 模型的 PyTorch 实现](https://juejin.im/post/5b9f1af0e51d450e425eb32d)

[哈佛大学的Transformer实现](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

[Details Need More Attention: Transformer 没有被提到的细节](https://zhuanlan.zhihu.com/p/79987949)

[NLPer看过来，一些关于Transformer的问题整理](https://www.nowcoder.com/discuss/258321?type=post&order=hot&pos=&page=1)

[Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

[transformer中的attention为什么scaled?](https://www.zhihu.com/question/339723385)

[为什么Transformer 需要进行 Multi-head Attention？](https://www.zhihu.com/question/341222779)

#### 训练技巧

[对Focal Loss的认识](http://skyhigh233.com/blog/2018/04/04/focalloss/)

## 词向量

[nlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert](https://zhuanlan.zhihu.com/p/56382372)

[doc2vec原理及实践](https://blog.csdn.net/John_xyz/article/details/79208564)

[GloVe与word2vec的区别](<https://zhuanlan.zhihu.com/p/31023929>)

[Word2vec数学原理全家桶](https://shomy.top/2017/07/28/word2vec-all/)

## NLP传统模型

[用TF特征向量和simhash指纹计算中文文本的相似度](<https://github.com/zyymax/text-similarity>)

[NLP点滴——文本相似度](https://www.cnblogs.com/xlturing/p/6136690.html)

[海量数据去重之SimHash算法简介和应用](https://blog.csdn.net/u010454030/article/details/49102565)

## Python进阶

[关于python的面试题](https://github.com/kenwoodjw/python_interview_question)

[python自测100题](https://zhuanlan.zhihu.com/p/57991045)

## 数学基础

[distribution-is-all-you-need](https://github.com/graykode/distribution-is-all-you-need)

## 比赛经验

[AI Challenger 2018 文本挖掘类竞赛相关解决方案及代码汇总](http://www.52nlp.cn/ai-challenger-2018-%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E7%B1%BB%E7%AB%9E%E8%B5%9B%E7%9B%B8%E5%85%B3%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E5%8F%8A%E4%BB%A3%E7%A0%81%E6%B1%87%E6%80%BB)

[Kaggle混分记](https://zhuanlan.zhihu.com/p/56747391)

[Kaggle QIQC比赛总结](https://zhuanlan.zhihu.com/p/57015732#comments)

[AI Challenger 2018 细粒度用户评论情感分析，排名17th](https://github.com/BigHeartC/Al_challenger_2018_sentiment_analysis)

## 大数据题

[海量数据中找出前k大数（topk问题）](https://zhuanlan.zhihu.com/p/119385223)

[教你如何迅速秒杀掉：99%的海量数据处理面试题](https://blog.csdn.net/v_JULY_v/article/details/7382693)

https://github.com/imhuay/Algorithm_Interview_Notes-Chinese)

## 智力题

[热门智力题 过桥问题和倒水问题](https://blog.csdn.net/MoreWindows/article/details/7481851)

[六道腾讯、百度、美团常爱问的面试智力题和答案](https://cloud.tencent.com/developer/article/1528465)